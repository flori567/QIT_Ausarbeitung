{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bibliotheken einlesen\n",
    "\n",
    "Für das Aufbereiten und Visualisieren, sowie für die Erstellung der Machine Learning Modelle werden verschiedene Bibliotheken benötigt. Folgende Python Bibiliotheken werden zur Durchführung der Analysen verwendet:\n",
    "\n",
    "**pandas**\n",
    "\n",
    "Mit der [**pandas**](https://pandas.pydata.org/) Bibiliothek können Daten aus verschiedenen Formaten in ein Data Frame eingelesen werden. Es stehen weiterhin Funktionen für die Datenbereinigung, für das Aggregieren oder Transformieren von Daten und anderen Dingen zur Verfügung.\n",
    "\n",
    "---\n",
    "\n",
    "**matplotlib**\n",
    "\n",
    "[**matplotlib**](https://matplotlib.org/) ist eine weit verbreitete Python Bibliothek mit der man verschiedene Charts erstellen kann. Sie bietet viele Konfigurationsmöglichkeiten, um auch komplexe Darstellungen zu ermöglichen\n",
    "\n",
    "---\n",
    "\n",
    "**seaborn**\n",
    "\n",
    "Die [**seaborn**](https://seaborn.pydata.org/) Bibliothek baut auf der Library matplotlib auf und ermöglicht es, mit einfacher Syntax anschauliche Datenvisualisierungen zu erzeugen. Im Vergleich zu matplotlib wirken die Grafiken moderner und sind mit weniger Kommandos zu erstellen.\n",
    "\n",
    "---\n",
    "\n",
    "**plotly**\n",
    "\n",
    "Die [**plotly**](https://plotly.com/) ..\n",
    "\n",
    "---\n",
    "\n",
    "**sklearn**\n",
    "\n",
    "Die [**sklearn**](https://scikit-learn.org/stable/) .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# For ML models\n",
    "from sklearn.linear_model import LinearRegression ,LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC ,SVR\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Überblick über die Daten erhalten\n",
    "\n",
    "Im nächsten Schritt wird sich ein Überblick über die Daten verschafft. Dabei werden die einzelnen Variablen betrachtet und beschrieben.\n",
    "\n",
    "Dazu werden die Daten aus der CSV-Datei *train.csv* in ein pandas DataFrame *df* eingelesen. Anschließend wird mit dem Aufruf des DataFrame-Namen eine Übersicht des Datasets ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart_2020_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Aus der Übersicht geht hervor, dass der Datensatz aus insgesamt 319795 Zeilen und 18 Spalten besteht. Sowohl die Anzahl Zeilen also auch die Anzahl der Spalten eignet sich sehr gut für die Entwicklung eines aussagekräftigen Modells. Für ein besseres Verständnis des Datasets werden im folgenden die Beschreibungen der Spalten aufgeführt:\n",
    "\n",
    "| Variable      | Beschreibung |\n",
    "| :-----------   | :-----------  |\n",
    "| **HeartDisease**  | **Zielvariable**. Befragte, die jemals eine koronare Herzkrankheit (KHK) oder einen Myokardinfarkt (MI) hatten   |\n",
    "| BMI     | Body Mass Index (BMI)         |\n",
    "| Smoking | Haben die Probanden in ihrem Leben mehr als 100 Zigaretten geraucht? |\n",
    "| AlcoholDrinking | Starke Trinker (erwachsene Männer mit mehr als 14 Getränken pro Woche und erwachsene Frauen mit mehr als 7 Getränken pro Woche) |\n",
    "| Stroke | Wurde dem Probanden jemals gesagt, dass er einen Schlaganfall hatte? |\n",
    "| PhysicalHealth | An wie viele der letzten 30 Tage hatten die Probanden physische körperliche Beschwerden |\n",
    "| MentalHealth | An wie viele der letzten 30 Tage hatten die Probanden psychische Beschwerden |\n",
    "| DiffWalking | Haben die Probanden starke Beschwerden, Treppen zu steigen? |\n",
    "| Sex | Geschlecht der Probanden |\n",
    "| AgeCategory | Alterskategorie (insgesamt 14 Kategorien) |\n",
    "| Race | Rasse / Ethnizität |\n",
    "| Diabetic | Hatte der Proband jemals Diabetes? |\n",
    "| PhysicalActivity | Hat der Proband in den letzten 30 Tagen Sport gemacht |\n",
    "| GenHealth | Wie schätzt der Proband seine Gesundheite ein -> Exzellent / Sehr gut / gut / angemessen / schlecht |\n",
    "| SleepTime | Durschnittliche Schlafzeit in Stunden |\n",
    "| Asthma | Hatte der Proband jemals Asthma? |\n",
    "| KidneyDisease | Wurde dem Probanden jemals gesagt, dass Sie eine Nierenerkrankung haben, ausgenommen Nierensteine, Blasenentzündung oder Inkontinenz? |\n",
    "| SkinCancer | Hatte der Proband jemals Hautkrebs? |\n",
    "\n",
    "\n",
    "Anhand der Daten der verschiedenen Spalten ist zu erkennen, dass das Dataset sowohl kategorische, als auch kontinuierliche Spalten besitzt. Für die unterschiedlichen Spaltentypen sind unterschiedliche Analysen und Visualisierungen sinnvoll. Daher wird im Folgenden eine **Aufteilung der Spalten in Arrays** vorgenommen, um sie im Anschluss getrennt analysieren zu können.\n",
    "\n",
    "Das Array *cat_cols* hält alle kategorischen Spalten, das Array *con_cols* alle kontinuierlichen Spalten.\n",
    "Zusätzlich wird ein Array angelegt mit der Zielvariable *HeartDisease*. Diese wird später für die Vorhersage des Preises eines Smartphones genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = ['Smoking','AlcoholDrinking','Stroke','DiffWalking','Sex','Race',\n",
    "            'Diabetic', 'PhysicalActivity', 'GenHealth', 'Asthma', 'KidneyDisease', 'SkinCancer']\n",
    "\n",
    "con_cols = [\"BMI\",\"PhysicalHealth\",\"MentalHealth\",\"AgeCategory\", \"SleepTime\"]\n",
    "\n",
    "target_col = [\"HeartDisease\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Bevor die einzelnen Spalten analysiert werden, werden zunächst noch einige Analysen über den Datensatz insgesamt vorgenommen. So werden beispielsweise die eindeutigen Werte pro Spalte betrachtet, oder ob Spalten vorliegen, die null-Werte enthalten.\n",
    "\n",
    "Ziel ist es, die Qualtät des Datensatzes zu ermitteln. Fehlende oder fälschliche Daten können im Verlauf der Analyse oder auch bei der Erstellung der Modelle zu schlechten oder falschen Ergebnissen führen.\n",
    "\n",
    "Hierzu wird eine Übersicht als Tabelle erstellt, in der der Datentyp, die fehlenden und eindeutigen Werte, sowie das Minimum und Maximum pro Spalte dargestellt wird. Zur Erstellung dieser Werte bietet die Bibliothek *pandas* einige hilfreiche Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Datentyp\n",
    "data_types = pd.DataFrame(\n",
    "    df.dtypes,\n",
    "    columns=['Datentyp']\n",
    ")\n",
    "\n",
    "## Fehlende Werte\n",
    "missing_data = pd.DataFrame(\n",
    "    df.isnull().sum(),\n",
    "    columns=['Fehlende Werte']\n",
    ")\n",
    "\n",
    "## Eindeutige Werte\n",
    "unique_values = pd.DataFrame(\n",
    "    columns=['Eindeutige Werte']\n",
    ")\n",
    "for row in list(df.columns.values):\n",
    "    unique_values.loc[row] = [df[row].nunique()]\n",
    "\n",
    "## Minimum\n",
    "minimum_values = pd.DataFrame(\n",
    "    columns=['Minimaler Wert']\n",
    ")\n",
    "for row in list(df.columns.values):\n",
    "    minimum_values.loc[row] = [df[row].min()]\n",
    "\n",
    "## Maximum\n",
    "maximum_values = pd.DataFrame(\n",
    "    columns=['Maximaler Wert']\n",
    ")\n",
    "for row in list(df.columns.values):\n",
    "    maximum_values.loc[row] = [df[row].max()]\n",
    "\n",
    "\n",
    "dq_report = data_types.join(missing_data).join(unique_values).join(minimum_values).join(maximum_values)\n",
    "dq_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu erkennen ist, dass die Spalte **AgeCategory** noch den Datentyp Object besitzt. Für eine einfachere Analyse der Daten, wird die Spalte in eine numerische Spalte konvertiert, indem jeweils die Mitte einer Alterskategorie als Wert ersetzt wird (z.B. 57 für den Eintrag \"55-59\"). \n",
    "\n",
    "So kann die Spalte auch besser in späteren Machine Learning verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AgeCategory zu einer kontinuierlichen Variable konvertieren\n",
    "\n",
    "## Zuordnung von AgeCategory Wert zu einem neuen Wert\n",
    "encode_AgeCategory = {'18-24': 21, '25-29': 27, '30-34': 32, '35-39': 37, '40-44': 42, '45-49': 47, '50-54': 52, '55-59': 57, \n",
    "                      '60-64': 62, '65-69': 67, '70-74': 72, '75-79': 77, '80 or older': 80}\n",
    "\n",
    "## Datensatz durchiterieren und Werte für AgeCategory ersetzen\n",
    "df['AgeCategory'] = df['AgeCategory'].apply(lambda x: encode_AgeCategory[x])\n",
    "\n",
    "## AgeCategory von einem 'object' Datentyp in einen 'float' Datentyp konvertieren\n",
    "df['AgeCategory'] = df['AgeCategory'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Folgende **Erkenntnisse** gehen aus der oberen Übersicht hervor:\n",
    "\n",
    "1. Nahezu alle Spalten besitzen den Datentyp int64. Lediglich die Spalten clock_speed und m_dep sind float64-Spalten. Das gesamte Dataset besteht demnach aus Zahlen. Es gibt also keine Text-Spalten oder ähnliches.\n",
    "\n",
    "2. Keine der Spalten weißt fehlende Werte auf. Im Dataset liegen demnach keine Einträge mit null-Werte vor.\n",
    "\n",
    "3. Es gibt Spalten im Dataset mit lediglich 2 eindeutigen Werte. Dies deutet auf Boolean-Spalten hin. Darüber hinaus gibt es Spalten mit nur wenigen Ausprägungen, die vermutlich kategorische Werte abbilden. Zuletzt gibt es Spalten mit sehr viele eindeutigen Werten, die auf kontinuierliche Werte hindeuten.\n",
    "\n",
    "4. Die Minium und Maximum Werte der Spalten geben den Wertebereich an.\n",
    "\n",
    "**Bewertung**\n",
    "\n",
    "Insgesamt weißt das Dataset eine sehr gute Qualität auf. Es liegen keine fehlenden Werte vor, die das Ergebnis der Vorhersage oder der Analysen beeinträchtigen könnten. Darüber hinaus können im Dataset ausschließlich mit Zahlen gearbeitet werden. Es muss also keine Konvertierung von Textspalten oder ähnliches vorgenommen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Unvariate Analyse der Spalten\n",
    "\n",
    "Nachdem eine Überblick über das Dataset gegeben wurde, werden nun Spalten des Dataset einzeln betrachtet und statistische Auswertungen erstellt. Diesen Vorgang nennt man auch \"Unvariate Analyse\". Es werden also noch keine Spalten in Beziehung gesetzt.\n",
    "Zu Analyse werden die kategorischen und kontinuierlichen Spalten gesondert betrachtet, wie bereits eingangs erwähnt. Es bieten sich pro Spaltentyp unterschiedliche Visualisierungen und Analysen an.\n",
    "\n",
    "Zunächst werden die **kategorischen Spalten** mithilfe von Bar-Charts betrachtet. Hierzu werden alle kategorischen Spalten in einer gemeinsamen Übersicht mithilfe von Subploty der Plotly-Bibliothek dargestellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kategorische Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=4, subplot_titles=(\"Smoking\", \"AlcoholDrinking\", \"Stroke\", \"DiffWalking\", \"Sex\",\n",
    "                                                    \"Race\", \"Diabetic\", \"PhysicalActivity\", \"GenHealth\", \"Asthma\",\n",
    "                                                    \"KidneyDisease\", \"SkinCancer\"))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['Smoking']),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['AlcoholDrinking']),\n",
    "              row=1, col=2)\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['Stroke']),\n",
    "              row=1, col=3)\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['DiffWalking']),\n",
    "              row=1, col=4)\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['Sex']),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['Race']),\n",
    "              row=2, col=2)\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['Diabetic']),\n",
    "              row=2, col=3)\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['PhysicalActivity']),\n",
    "              row=2, col=4)\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['GenHealth']),\n",
    "              row=3, col=1)\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['Asthma']),\n",
    "              row=3, col=2)\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['KidneyDisease']),\n",
    "              row=3, col=3)\n",
    "fig.add_trace(go.Histogram(histfunc=\"count\",  x=df['SkinCancer']),\n",
    "              row=3, col=4)\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"Kategorische Variablen\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kontinuierliche Variablen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Für die Betrachtung der kontinuierlichen Variablen bieten sich sowohl Histogramme und auch Boxplots an. Boxplots sind Diagramme, die zur graphischen Darstellung der Verteilung von mindestens ordinalskalierten Merkmalen verwendet werden. Es fasst dabei verschiedene Streuungs- und Lagemaße in einer Darstellung zusammen. Ein Box-Plot soll schnell einen Eindruck darüber vermitteln, in welchem Bereich die Daten liegen und wie sie sich über diesen Bereich verteilen.\n",
    "\n",
    "Ein Boxplot fasst folgende Werte in einer Darstellung zusammen:\n",
    "- Minimum\n",
    "- Unteres Quartil\n",
    "- Median\n",
    "- Oberes Quartil\n",
    "- Maximum\n",
    "- Spannweite\n",
    "- Interquartilsabstand (IQR)\n",
    "\n",
    "Darüber hinaus werden extreme Ausreißer (3x IQR) in den Daten als Punkte dargestellt.\n",
    "\n",
    "Ein Histogramm liefert darüber hinaus die grafische Darstellung der absoluten oder relativen Häufigkeitsverteilung eines quantitativen, klassierten Merkmals in einem speziellen Säulendiagramm: Der Flächeninhalt der einzelnen (aneinandergrenzenden) Säulen gibt die Häufigkeit der jeweiligen Klassen wider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## BMI\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(histfunc=\"count\",  x=df['BMI']),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(x=df['BMI']),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, width=1000, title_text=\"Body Mass Index\", showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BMI\n",
    "sns.set(rc={'figure.figsize':(26,9.27)})\n",
    "\n",
    "plt.subplot(231)\n",
    "sns.boxplot(data=df, x=\"BMI\")\n",
    "plt.subplot(232)\n",
    "sns.histplot(data=df, x=\"BMI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PhysicalHealth\n",
    "\n",
    "plt.subplot(231)\n",
    "sns.boxplot(data=df, x=\"PhysicalHealth\")\n",
    "plt.subplot(232)\n",
    "sns.countplot(x=df[\"PhysicalHealth\"].astype(int), color='#5975a4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## MentalHealth\n",
    "\n",
    "plt.subplot(231)\n",
    "sns.boxplot(data=df, x=\"MentalHealth\")\n",
    "plt.subplot(232)\n",
    "sns.countplot(x=df[\"MentalHealth\"].astype(int), color='#5975a4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## AgeCategory\n",
    "\n",
    "plt.subplot(231)\n",
    "sns.boxplot(data=df, x=\"AgeCategory\")\n",
    "plt.subplot(232)\n",
    "sns.countplot(x=df[\"AgeCategory\"].astype(int), color='#5975a4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SleepTime\n",
    "\n",
    "plt.subplot(231)\n",
    "sns.boxplot(data=df, x=\"SleepTime\")\n",
    "plt.subplot(232)\n",
    "sns.countplot(x=df[\"SleepTime\"].astype(int), color='#5975a4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zielvariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(16, 5.27)})\n",
    "sns.countplot(x=\"HeartDisease\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bivariate Analyse der Spalten\n",
    "\n",
    "Im Anschluss an die unvariate Analyse folgt die bivariate Analyse der Spalten. Der Fokus liegt hier in der Suche nach Beziehungen zwischen 2 oder mehr Spalten. Genauer gesagt ist das Ziel, eine Beziehung zischen der Zielvariabel *HeartDisease* und einer beschreibenden Variable wie z.B. *Smoking* zu finden. Es wird also eine Art empirische Beziehung zwischen den Variablen ermittelt.\n",
    "\n",
    "Außerdem werden Korrelationen zwischen den beschreibenden Variablen ermittelt. Eine hohe Korrelation zweier Variablen sagt aus, dass die eine Variable die andere bedingt.\n",
    "\n",
    "Vor der bivariaten Analyse werden allerdings noch Hypothesen aufgestellt, die eine Richtung bei der Analyse der Daten vorgeben. Man stellt also Vermutungen zu den Daten auf, die im Anschluss mittels Analysen und Visualisierung belegt oder wiederlegt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Hypthesen**:\n",
    "\n",
    "1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kategorische Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(16, 9.27)})\n",
    "\n",
    "plt.subplot(231)\n",
    "sns.countplot(x=\"Smoking\", hue='HeartDisease', data=df)\n",
    "plt.subplot(232)\n",
    "sns.countplot(x=\"AlcoholDrinking\", hue='HeartDisease', data=df)\n",
    "plt.subplot(233)\n",
    "sns.countplot(x=\"Stroke\", hue='HeartDisease', data=df)\n",
    "\n",
    "variables = [\"Smoking\", \"AlcoholDrinking\", \"Stroke\"]\n",
    "\n",
    "for v in variables:\n",
    "    smoke_and_heart_disease = len(df[(df['HeartDisease']=='Yes') & (df[v]=='Yes')])\n",
    "    num_smoke = len(df[df[v]=='Yes'])\n",
    "    no_smoke_and_heart_disease = len(df[(df['HeartDisease']=='Yes') & (df[v]=='No')])\n",
    "    num_no_smoke = len(df[df[v]=='No'])\n",
    "    print('Wahrscheinlichkeit für einen Herzinfakt, wenn die Ausprägung ' + v + ' \"Yes\" ist:', \n",
    "          str(round(smoke_and_heart_disease * 100 / num_smoke, 2)) + \" %\")\n",
    "    print('Wahrscheinlichkeit für einen Herzinfakt, wenn die Ausprägung ' + v + ' \"No\" ist:', \n",
    "          str(round(no_smoke_and_heart_disease * 100 /num_no_smoke, 2)) + \" %\")\n",
    "    print('-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(234)\n",
    "sns.countplot(x=\"DiffWalking\", hue='HeartDisease', data=df)\n",
    "plt.subplot(235)\n",
    "sns.countplot(x=\"Sex\", hue='HeartDisease', data=df)\n",
    "plt.subplot(236)\n",
    "sns.countplot(x=\"Race\", hue='HeartDisease', data=df)\n",
    "\n",
    "## DiffWalking\n",
    "smoke_and_heart_disease = len(df[(df['HeartDisease']=='Yes') & (df['DiffWalking']=='Yes')])\n",
    "num_smoke = len(df[df['DiffWalking']=='Yes'])\n",
    "no_smoke_and_heart_disease = len(df[(df['HeartDisease']=='Yes') & (df['DiffWalking']=='No')])\n",
    "num_no_smoke = len(df[df['DiffWalking']=='No'])\n",
    "print('Wahrscheinlichkeit für einen Herzinfakt, wenn die Ausprägung DiffWalking \"Yes\" ist:', \n",
    "      str(round(smoke_and_heart_disease * 100 / num_smoke, 2)) + \" %\")\n",
    "print('Wahrscheinlichkeit für einen Herzinfakt, wenn die Ausprägung DiffWalking \"No\" ist:', \n",
    "      str(round(no_smoke_and_heart_disease * 100 /num_no_smoke, 2)) + \" %\")\n",
    "print('-------------')\n",
    "\n",
    "## Sex\n",
    "smoke_and_heart_disease = len(df[(df['HeartDisease']=='Yes') & (df['Sex']=='Male')])\n",
    "num_smoke = len(df[df['Sex']=='Male'])\n",
    "no_smoke_and_heart_disease = len(df[(df['HeartDisease']=='Yes') & (df['Sex']=='Female')])\n",
    "num_no_smoke = len(df[df['Sex']=='Female'])\n",
    "print('Wahrscheinlichkeit für einen Herzinfakt, wenn die Ausprägung Sex \"Male\" ist:', \n",
    "      str(round(smoke_and_heart_disease * 100 / num_smoke, 2)) + \" %\")\n",
    "print('Wahrscheinlichkeit für einen Herzinfakt, wenn die Ausprägung Sex \"Female\" ist:', \n",
    "      str(round(no_smoke_and_heart_disease * 100 /num_no_smoke, 2)) + \" %\")\n",
    "print('-------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(234)\n",
    "sns.countplot(x=\"Diabetic\", hue='HeartDisease', data=df)\n",
    "plt.subplot(235)\n",
    "sns.countplot(x=\"PhysicalActivity\", hue='HeartDisease', data=df)\n",
    "plt.subplot(236)\n",
    "sns.countplot(x=\"GenHealth\", hue='HeartDisease', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(234)\n",
    "sns.countplot(x=\"Asthma\", hue='HeartDisease', data=df)\n",
    "plt.subplot(235)\n",
    "sns.countplot(x=\"KidneyDisease\", hue='HeartDisease', data=df)\n",
    "plt.subplot(236)\n",
    "sns.countplot(x=\"SkinCancer\", hue='HeartDisease', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kontinuierliche Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df, x=\"BMI\", hue=\"HeartDisease\", alpha=0.5, shade=True, multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df, x=\"PhysicalHealth\", hue=\"HeartDisease\", alpha=0.5, shade=True, multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df, x=\"MentalHealth\", hue=\"HeartDisease\", alpha=0.5, shade=True, multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df, x=\"AgeCategory\", hue=\"HeartDisease\", alpha=0.5, shade=True, multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df, x=\"SleepTime\", hue=\"HeartDisease\", alpha=0.5, shade=True, multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zielvariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[df['HeartDisease']=='Yes'], x=\"BMI\", kde=True, color=\"#ea4335\")\n",
    "\n",
    "sns.histplot(df[df['HeartDisease']=='No'], x=\"BMI\", kde=True, color='#4285f4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Korrelationsmatrix aller Variablen erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[['HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex',\n",
    "                    'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer','BMI', 'PhysicalHealth',\n",
    "                    'MentalHealth', 'AgeCategory', 'SleepTime']].corr()\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "gs = fig.add_gridspec(1,1)\n",
    "gs.update(wspace=0.3, hspace=0.15)\n",
    "ax0 = fig.add_subplot(gs[0,0])\n",
    "\n",
    "color_palette = [\"#5833ff\",\"#da8829\"]\n",
    "mask = np.triu(np.ones_like(df_corr))\n",
    "ax0.text(1.5,-0.1,\"Correlation Matrix\",fontsize=22, fontweight='bold', fontfamily='serif', color=\"#000000\")\n",
    "df_corr = df[['HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex',\n",
    "                    'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer','BMI', 'PhysicalHealth',\n",
    "                    'MentalHealth', 'AgeCategory', 'SleepTime']].corr().transpose()\n",
    "sns.heatmap(df_corr,mask=mask,fmt=\".2f\",annot=True,cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df.corr().round(2)\n",
    "sns.heatmap(correlation,annot=True ,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Modell erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bislang wurden die Daten analysiert mithilfe von verschiedenen Visualisierungen. Dabei wurde ein Verständnis für die Daten entwickelt.\n",
    "Nun soll für die Vorhersage der Zielvariable **HeartDisease** ein bzw. mehrere Machine Learning Modelle erstellt werden. Dabei werden verschiedene Verfahren betrachtet, um zu ermitteln, welches der Verfahren die besten Ergebnisse erzielt. Dies ist ein gängiges Vorgehen im Data Science Bereich, da sich die Modelle je nach Datensatz unterschiedlich verhalten können.\n",
    "\n",
    "Bevor die Machine Learning Modelle allerdings erstellt werden, werden die Daten noch aufbereitet, um die spätere Genauigkeit der Vorhersagen zu verbessern. Im Folgenden werden dafür 2 Verfahren angewendet: **StandardScaler** und **OneHotEncoding**.\n",
    "\n",
    "StandardScaler:\n",
    "Kontinuierliche Variablen in einem Dataset weisen oft unterschiedliche Skalen auf. So hat die Variable **BMI** beispielsweise eine Skala von 12.02 bis 94.85, die Variable **SleepTime** hingegen eine Skala von 1 bis 24. Dies kann die Vorhersageperformance viele Machine Learning Modelle beeinträchtigen. Nicht skalierte Daten können auch die Konvergenz vieler gradientenbasierter Modelle verlangsamen oder sogar verhindern.\n",
    "\n",
    "<div>\n",
    "<img src=\"StandardScaler.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "OneHotEncoding:\n",
    "Viele Machine Learning Modelle können nur schwer mit kategorischen Daten als Input umgehen. Ähnlich wie bei den kontinuierlich Variablen ist auch hier das Problem, dass einige Variablen nur 2 Ausprägungen besitzen, andere dafür aber 20 oder mehr. \n",
    "Viel besser können Machine Learning Modelle mit numerischen Werte umgehen.\n",
    "\n",
    "Beim OneHotEncoding wird jeder kategorische Wert in eine neue kategorische Spalte umgewandelt und diesen Spalten ein binärer Wert von 1 oder 0 zugewiesen. Jeder ganzzahlige Wert wird als binärer Vektor dargestellt. Alle Werte sind Null, und der Index ist mit einer 1 gekennzeichnet.\n",
    "\n",
    "<div>\n",
    "<img src=\"one_hot_encoding.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisierung der Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Standardisierung der kontinuierlichen Variablen\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "df[con_cols] = Scaler.fit_transform(df[con_cols])\n",
    "\n",
    "df[con_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach Anwendung des StandardScaler haben alle kontinuierlichen Variabeln einen Durchschnitt von 0 (µ = 0) und eine Standardabweichung von 1 (σ = 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer encode columns with 2 unique values\n",
    "for col in ['HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex', 'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer']:\n",
    "    if df[col].dtype == 'O':\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# One-hot encode columns with more than 2 unique values\n",
    "df = pd.get_dummies(df, columns=['Race', 'Diabetic', 'GenHealth', ], prefix = ['Race', 'Diabetic', 'GenHealth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach Anwendung des OneHotEncoding gibt es neue Spalten wie z.B. die Spalte **Race_White**, die die Ausprägungen 0 und 1 besitzt. Für alle kategorischen Werte der Spalten 'Race', 'Diabetic' und 'GenHealth' gibt es nun eine dedizierte Spalte mit den Werte 0 und 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die letzten Vorbereitung der Daten für die Machine Learning Modelle vorgenommen wurden, werden die Daten nun in Trainings- und Testdaten unterteilt. Dies ist ebenfalls ein weit verbreitetes Vorgehen im Data Science Umfeld.\n",
    "Die Idee ist, das Modell auf einem Teil der Daten zu trainieren (Trainingsdaten) und das Modell anschließend auf einem Datensatz zu testen, dass das Modell zuvor noch nicht gesehen hat (Testdaten). \n",
    "\n",
    "Eine gängige Aufteilung ist hierbei 80/20, wobei 80% Trainingsdaten und 20% Testdaten verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split dataset for training and testing\n",
    "\n",
    "#Select Features\n",
    "features = df.drop(columns =['HeartDisease'], axis = 1)\n",
    "\n",
    "#Select Target \n",
    "target = df['HeartDisease']\n",
    "\n",
    "# Set Training and Testing Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, shuffle = True, test_size = .2, random_state = 44)\n",
    "\n",
    "\n",
    "print('Shape of training feature:', X_train.shape)\n",
    "print('Shape of testing feature:', X_test.shape)\n",
    "print('Shape of training label:', y_train.shape)\n",
    "print('Shape of training label:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für alle ausgewählten Modelle soll nach dem Training eine Validierung der Genauigkeit erfolgen. Da diese Validierung immer nach demselben Schema erfolgt und dieselben Metriken betrachtet werden sollen, wird im folgenden eine Funktion *evaluate_model* erstellt, mit der die Modelle später einfacher validiert werden können, da die Ergebnisse in einer standardisierten Form vorliegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test):\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # Predict Test Data \n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate accuracy, precision, recall, f1-score, and kappa score\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    prec = metrics.precision_score(y_test, y_pred)\n",
    "    rec = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate area under curve (AUC)\n",
    "    y_pred_proba = model.predict_proba(x_test)[::,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Display confussion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'kappa': kappa, \n",
    "            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a model using KNeighborsClassifier \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Evaluate Model\n",
    "knn_eval = evaluate_model(knn, X_test, y_test)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', knn_eval['acc'])\n",
    "print('Precision:', knn_eval['prec'])\n",
    "print('Recall:', knn_eval['rec'])\n",
    "print('F1 Score:', knn_eval['f1'])\n",
    "print('Cohens Kappa Score:', knn_eval['kappa'])\n",
    "print('Area Under Curve:', knn_eval['auc'])\n",
    "print('Confusion Matrix:\\n', knn_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Building Decision Tree model \n",
    "# https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 3, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "clf_eval = evaluate_model(clf, X_test, y_test)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', clf_eval['acc'])\n",
    "print('Precision:', clf_eval['prec'])\n",
    "print('Recall:', clf_eval['rec'])\n",
    "print('F1 Score:', clf_eval['f1'])\n",
    "print('Cohens Kappa Score:', clf_eval['kappa'])\n",
    "print('Area Under Curve:', clf_eval['auc'])\n",
    "print('Confusion Matrix:\\n', clf_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "tree.plot_tree(clf,\n",
    "               feature_names = df.columns,\n",
    "               class_names = target.astype(str),\n",
    "               filled = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building RandomForest classifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "rfc_eval = evaluate_model(rfc, X_test, y_test)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', rfc_eval['acc'])\n",
    "print('Precision:', rfc_eval['prec'])\n",
    "print('Recall:', rfc_eval['rec'])\n",
    "print('F1 Score:', rfc_eval['f1'])\n",
    "print('Cohens Kappa Score:', rfc_eval['kappa'])\n",
    "print('Area Under Curve:', rfc_eval['auc'])\n",
    "print('Confusion Matrix:\\n', rfc_eval['cm'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "<div>\n",
    "<img src=\"Support_Vector_Machine.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the object and fitting\n",
    "# https://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "svc = SVC(kernel='linear', C=1, random_state=42).fit(X_train,y_train)\n",
    "\n",
    "# predicting the values\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# printing the test accuracy\n",
    "print(\"The test accuracy score of SVM is \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import warnings\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(256, activation = 'relu', input_shape=(X_train.shape[1],)))\n",
    "classifier.add(Dense(515, activation = 'relu'))\n",
    "classifier.add(Dropout(0.3))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(50, activation = 'relu'))\n",
    "classifier.add(Dropout(0.3))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)\n",
    "classifier.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " history = classifier.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 30, batch_size=15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred.round())\n",
    "sns.heatmap(cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n",
    "#accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "ac=accuracy_score(y_test, y_pred.round())\n",
    "print('accuracy of the model: ',ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Losss\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
